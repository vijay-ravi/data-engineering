{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3863cea-ec28-41ce-a8fb-21b3d0c2f0f5",
      "metadata": {
        "id": "a3863cea-ec28-41ce-a8fb-21b3d0c2f0f5"
      },
      "source": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vijay-ravi/data-engineering/blob/main/netflix/01_AWS_data_project_netflix.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFwMl1PEyVJv"
      },
      "source": [
        "\n",
        "# AWS: Building a basic ETL pipeline with AWS Glue and S3\n",
        "\n",
        "1. **Import Libraries**\n",
        "2. **Enter AWS Credentials**\n",
        "  - Here we initialize credentials we can pass to boto3.\n",
        "3. **Assign bucket name**\n",
        "  - S3 bucket names should be unique globally.\n",
        "4. **Initialize boto3**\n",
        "  - Boto3 is the AWS SDK for Python.\n",
        "5. **Create IAM role for glue**\n",
        "  - IAM stands for Identity Access and Management.\n",
        "  - In order for Glue to access S3 resources, we need to assign it a role to do that.\n",
        "6. **Create S3 bucket and upload files to S3**\n",
        "  - S3 stands for Simple Storage Service.\n",
        "  - Here we upload the files for netflix.\n",
        "7. **Create Glue pyspark script and upload to S3**\n",
        "  - Here we create a pyspark script which aggregates the episodes by year.\n",
        "8. **Create AWS Glue Job**\n",
        "  - AWS Glue is an ETL (Extract Transform Load) service.\n",
        "  - We can run both python and pyspark scripts on it.\n",
        "  - It is serverless, which means the servers are managed by AWS.\n",
        "9. **Run AWS Glue Job**\n",
        "10. **Cleanup S3 and Glue**\n",
        "\n",
        "\n",
        "Tutorial created by @[vijayxtech](https://beacons.ai/vijayxtech). Follow for more projects and guidance. **🤖**\n",
        "\n",
        "![picture](https://img1.wsimg.com/isteam/ip/9ef33804-39fa-4619-913a-87a9f3820fde/alchemyrefiner_alchemymagic_0_36045e9a-de11-4.jpeg/:/cr=t:0%25,l:0%25,w:100%25,h:100%25/rs=w:1280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGkpu954codZ"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWiWAbbmzjum",
        "outputId": "0e64c011-761c-41fc-b340-04c7472c4b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install -q boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXtSSc5fg9f6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import random\n",
        "from botocore.exceptions import NoCredentialsError, ClientError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI7t60qoh9v4"
      },
      "source": [
        "## 2. Enter AWS Credentials\n",
        "\n",
        "> Create access key and secret access key here:<br>\n",
        " https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGC3l66h-2n"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id='123'\n",
        "aws_secret_access_key='123'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlTTVyictsa"
      },
      "source": [
        "## 3. Initialize bucket name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NarcfOam80pI"
      },
      "outputs": [],
      "source": [
        "random_number = random.randint(10000, 99999)\n",
        "bucket_name = f'aws-data-projects-{random_number}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhqYyrfQczHm"
      },
      "source": [
        "## 4. Initialize boto3 and other variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCum5y60jLW8"
      },
      "outputs": [],
      "source": [
        "## create access key and secret access key here:\n",
        "## https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials\n",
        "\n",
        "# Initialize the boto3 client\n",
        "\n",
        "region='us-east-1'\n",
        "role_name='glue-service-role'\n",
        "glue_policy_arn='arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'\n",
        "s3_policy_arn = 'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
        "policy_arns = [glue_policy_arn, s3_policy_arn]\n",
        "\n",
        "\n",
        "file_paths = [\n",
        "    'netflix_india_shows_seasons.csv'\n",
        "]\n",
        "file_keys = [\n",
        "    'netflix/raw_data/netflix_india_shows_seasons.csv'\n",
        "]\n",
        "\n",
        "session = boto3.Session(region_name=region,\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,)\n",
        "\n",
        "# Initialize the boto3 clients\n",
        "s3_client = session.client(\n",
        "    's3',\n",
        ")\n",
        "\n",
        "glue_client = session.client(\n",
        "    'glue',\n",
        ")\n",
        "\n",
        "iam_client = session.client('iam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FuD7Auc48O"
      },
      "source": [
        "## 5. Create IAM role for glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI1fsNAo4LuU",
        "outputId": "20448547-6b5a-4c01-8b8b-07b35760926c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Role 'glue-service-role' already exists. Moving on to attaching policies.\n",
            "Policy arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole successfully attached to role glue-service-role.\n",
            "Policy arn:aws:iam::aws:policy/AmazonS3FullAccess successfully attached to role glue-service-role.\n"
          ]
        }
      ],
      "source": [
        "### CREATE GLUE IAM ROLE ####\n",
        "\n",
        "trust_policy = '''{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Principal\": {\n",
        "        \"Service\": \"glue.amazonaws.com\"\n",
        "      },\n",
        "      \"Action\": \"sts:AssumeRole\"\n",
        "    }\n",
        "  ]\n",
        "}'''\n",
        "\n",
        "# Step 1: Try to create the IAM role\n",
        "try:\n",
        "    iam_client.create_role(\n",
        "        RoleName=role_name,\n",
        "        AssumeRolePolicyDocument=trust_policy,\n",
        "        Description='Role for AWS Glue Service with S3 full-access policy.',\n",
        "    )\n",
        "    print(f\"Role '{role_name}' successfully created.\")\n",
        "except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "    print(f\"Role '{role_name}' already exists. Moving on to attaching policies.\")\n",
        "except Exception as error:\n",
        "    print(f\"Error creating role '{role_name}': {error}\")\n",
        "\n",
        "\n",
        "# Step 2: Attaching IAM managed policies to the IAM role\n",
        "for policy_arn in policy_arns:\n",
        "    try:\n",
        "        iam_client.attach_role_policy(\n",
        "            RoleName=role_name,\n",
        "            PolicyArn=policy_arn\n",
        "        )\n",
        "        print(f\"Policy {policy_arn} successfully attached to role {role_name}.\")\n",
        "    except Exception as error:\n",
        "            print(f\"Error attaching policy {policy_arn} to role '{role_name}': {error}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUPx3fAdAlQ"
      },
      "source": [
        "## 6. Create S3 bucket and upload files to S3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElqpnCn49DAA",
        "outputId": "5b8c73d9-8856-4ce7-d628-7d0e3defd560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bucket 'aws-data-projects-23721' created successfully.\n",
            "Uploaded netflix/raw_data/netflix_india_shows_seasons.csv to S3 bucket aws-data-projects-23721\n"
          ]
        }
      ],
      "source": [
        "## Create S3 bucket\n",
        "\n",
        "bucket_exists = True\n",
        "\n",
        "s3 = boto3.resource('s3',aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,)\n",
        "\n",
        "try:\n",
        "    s3.meta.client.head_bucket(Bucket=bucket_name)\n",
        "    print(f\"Bucket '{bucket_name}' already exists.\")\n",
        "except ClientError as e:\n",
        "    error_code = int(e.response['Error']['Code'])\n",
        "    if error_code == 404:\n",
        "        bucket_exists = False\n",
        "\n",
        "if not bucket_exists:\n",
        "    try:\n",
        "        if region is None:\n",
        "            s3_client.create_bucket(Bucket=bucket_name)\n",
        "        else:\n",
        "            location = {'LocationConstraint': region}\n",
        "            s3_client.create_bucket(Bucket=bucket_name)\n",
        "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
        "    except ClientError as e:\n",
        "        print(f\"Failed to create bucket: {e}\")\n",
        "\n",
        "\n",
        "# Upload files to S3\n",
        "\n",
        "try:\n",
        "    for file_path, file_key in zip(file_paths, file_keys):\n",
        "        s3_client.upload_file(file_path, bucket_name, file_key)\n",
        "        print(f\"Uploaded {file_key} to S3 bucket {bucket_name}\")\n",
        "except NoCredentialsError:\n",
        "    print(\"Credentials not available\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeLWuaKNdEtq"
      },
      "source": [
        "## 7. Create Glue pyspark script and upload to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIzQNn5wjLlS",
        "outputId": "246311ef-4b30-4fe9-e6bb-a8f718f9b5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded netflix_data_transformation.py to S3 bucket aws-data-projects-23721\n"
          ]
        }
      ],
      "source": [
        "###### GLUE SCRIPT AND JOB ############\n",
        "\n",
        "# S3 bucket and script file details\n",
        "script_file_name = 'netflix_data_transformation.py'\n",
        "s3_script_location = f's3://{bucket_name}/netflix/scripts/{script_file_name}'\n",
        "\n",
        "# AWS Glue job details\n",
        "job_name = 'NetflixIndiaDataTransformation'\n",
        "\n",
        "# PySpark script as a string\n",
        "pyspark_script = f\"\"\"\n",
        "import sys\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from awsglue.transforms import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "output_dir = \"s3://{bucket_name}/netflix/transformed-data/\"\n",
        "\n",
        "# Define the S3 paths for your datasets\n",
        "seasons_table_s3_path = \"s3://{bucket_name}/netflix/raw_data/netflix_india_shows_seasons.csv\"\n",
        "\n",
        "# Practice datasets\n",
        "# movies_table_s3_path = \"s3://{bucket_name}/netflix/raw_data/netflix_india_shows_and_movies.csv\"\n",
        "# episodes_table_s3_path = \"s3://{bucket_name}/netflix/raw_data/netflix_india_shows_episodes.csv\"\n",
        "\n",
        "# Adjust the read method to directly load from S3 paths\n",
        "seasons_df = spark.read.option(\"header\", \"true\").option(\"sep\",\",\").csv(seasons_table_s3_path)\n",
        "\n",
        "episodes_per_year = seasons_df.groupBy(\"release_year\").agg(_sum(\"episode_count\").alias(\"total_episodes\"))\n",
        "\n",
        "episodes_per_year.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"encoding\", \"UTF-8\") \\\n",
        "    .option(\"escape\", \"'\") \\\n",
        "    .option(\"quoteAll\", \"true\") \\\n",
        "    .csv(output_dir)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Script execution completed.\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Function to save and upload the PySpark script to S3\n",
        "def upload_script_to_s3(script_content, bucket, file_name):\n",
        "    # Save the script to a file\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(script_content)\n",
        "\n",
        "    # Upload the file to S3\n",
        "    try:\n",
        "        s3_client.upload_file(file_name, bucket, f'netflix/scripts/{script_file_name}')\n",
        "        print(f\"Uploaded {file_name} to S3 bucket {bucket}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        os.remove(file_name)\n",
        "\n",
        "\n",
        "upload_script_to_s3(pyspark_script, bucket_name, script_file_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-W7HdVqdKoq"
      },
      "source": [
        "## 8. Create AWS Glue Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbxOG0TIjLn_",
        "outputId": "73f468ec-18ee-4911-fa55-bd0cd9da9ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Glue job 'NetflixIndiaDataTransformation' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Function to create an AWS Glue job\n",
        "def create_glue_job(job_name, script_location, role):\n",
        "    try:\n",
        "        response = glue_client.create_job(\n",
        "            Name=job_name,\n",
        "            Role=role_name,\n",
        "            ExecutionProperty={'MaxConcurrentRuns': 1},\n",
        "            Command={\n",
        "                'Name': 'glueetl',\n",
        "                'ScriptLocation': script_location,\n",
        "                'PythonVersion': '3'\n",
        "            },\n",
        "            DefaultArguments={\n",
        "                '--TempDir': f's3://{bucket_name}/temp/',\n",
        "                '--job-bookmark-option': 'job-bookmark-enable',\n",
        "            },\n",
        "            MaxRetries=0,\n",
        "            Timeout=60,\n",
        "            GlueVersion='3.0',\n",
        "            WorkerType='G.1X',\n",
        "            NumberOfWorkers=2\n",
        "        )\n",
        "        print(f\"Glue job '{job_name}' created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "create_glue_job(job_name, s3_script_location, role_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8eZpBQdPUX"
      },
      "source": [
        "## 9. Run the Glue job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gC4an5_wjLqk",
        "outputId": "d435c4fc-7be7-4a44-bf39-ef296c13b931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Glue job 'NetflixIndiaDataTransformation' started. Job run ID: jr_cff19611725a262767c270b22c30c05a56a62982ae6566ba18e311e1400e50a9\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jr_cff19611725a262767c270b22c30c05a56a62982ae6566ba18e311e1400e50a9'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to start an AWS Glue job by name\n",
        "def start_glue_job(job_name):\n",
        "    try:\n",
        "        # Start the job\n",
        "        response = glue_client.start_job_run(JobName=job_name)\n",
        "        job_run_id = response['JobRunId']\n",
        "        print(f\"Glue job '{job_name}' started. Job run ID: {job_run_id}\")\n",
        "        return job_run_id\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred starting the Glue job '{job_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "start_glue_job(job_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZddrLfmTeP6I"
      },
      "source": [
        "## 10. Cleanup S3 and Glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR9tuaMojLtM"
      },
      "outputs": [],
      "source": [
        "def empty_s3_bucket(bucket_name):\n",
        "    \"\"\"\n",
        "    Empty all objects in an S3 bucket.\n",
        "    :param bucket_name: str. Name of the S3 bucket to be emptied.\n",
        "    \"\"\"\n",
        "    s3_resource = session.resource('s3')\n",
        "    bucket = s3_resource.Bucket(bucket_name)\n",
        "    s3_client = session.client('s3')\n",
        "    try:\n",
        "        # Delete all objects in the bucket\n",
        "        bucket.objects.all().delete()\n",
        "        print(f\"All objects in bucket '{bucket_name}' have been deleted.\")\n",
        "        s3_client.delete_bucket(Bucket=bucket_name)\n",
        "        print(f\"Bucket '{bucket_name}' deleted successfully.\")\n",
        "    except ClientError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def delete_glue_job(job_name):\n",
        "    \"\"\"\n",
        "    Delete an AWS Glue job.\n",
        "    :param job_name: str. Name of the Glue job to be deleted.\n",
        "    \"\"\"\n",
        "    glue = session.client('glue')\n",
        "    try:\n",
        "        # Delete the Glue job\n",
        "        glue.delete_job(JobName=job_name)\n",
        "        print(f\"Glue job '{job_name}' has been deleted.\")\n",
        "    except ClientError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Call functions to empty bucket and delete Glue job\n",
        "empty_s3_bucket(bucket_name)\n",
        "delete_glue_job(job_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpxYqEymjLvs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWN4Xzv-jLyU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "che6EXeBjL0l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
